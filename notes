-------------------------------------------------------------------------------------------------------
INTRODUCTION:
-------------------------------------------------------------------------------------------------------
Basics of docker:
    Dockerfile
    docker build
    docker pull
    dockr push 

-------------------------------------------------------------------------------------------------------
REGISTRY:
-------------------------------------------------------------------------------------------------------
It consists of Container registry and Artifact registry. 

Container registry:
    It is of host-name/project-id/image-name:tag 
    Host name is not specific to region.
    No IAM role is tagged.
    Host name is,
      gcr.io - Stores in United states data center.
      asia.gcr.io - Stores in asia data center.
      eu.gcr.io - Stores in EU data center.
      us.gcr.io - Stores in Unites states data center.
    $DEVSHELL_PROJECT_ID  - Environment variable to get current project id. 
    Internally creates bucket to store registry.
      
Artifact registry:
    It is advance to container registry.
    It supports roles. (reader, Writer, Administrator & repo administrator created by default)
    It isused to store images, artifacts like jfrog artifactory.
    Multiple repositories can create per project.
    Can create regional or multi regional level repositories.
    URL is us-central1-docker.pkg.dev/$PROJECT_ID/repo/image:tag
            us-central-1 can change depends upon region selected.    
    Configuration: 
        gcloud auth configure-docker us-central1-docker.pkg.dev  -> To configure docker to use google cloud cli to authenticate request to artifact registry in us-central.

-------------------------------------------------------------------------------------------------------
DEPLOY APPLICATIONS:
-------------------------------------------------------------------------------------------------------

Compute Options:

    Compute Engine: Like ec2.
    Kubernetes: like eks. Used to deploy containarized application. 
    App Engine: Serverless web application deployment.
    Cloud Run: Used to deploy containerized application.
    Cloud Function: Event triggered based function. 
    
Deployment Method:

    Blue/green
    Canary
    Rolling 
    Traffic Splitting: Send traffic to small percentage of users first (5% to v2 and 95% to v1 stable). It is used for A/B testing.

Google Cloud Function:

    It is event based trigger. 
        Events can be http, pub/sub, storage, firestore.
    Only one version active at any time, so it cannot roll back to previous version if any failure.
    To overcome this, require to create new functions and loadbalance it.
    
Google App Engine:

    It is used to deploy web application.
    It is earliest product and serverless. 
    Only one app engine can be deployed for one project.
    Environmant consists of,
        Standard
        Flexible    -   CUsom runtime.
    To deploy application, it requires,
        requirements.txt (for python)
        main.py (for python)
        app.yaml 
    Commands: 
        gcloud app deploy       -> deploy application and promote version.
        gcloud app deploy --no-promote --version name     -> deploy application code but not implement. 
    In Version, can Migrate Traffic (100%) and Split Traffic (partial)

Cloud Run:

    It introduced recently.
    Serverless 
    Its better version of app engine.
    Deploy containarized application.
    It is like ECS (to create a service)
    It is like AWS Batch (to create a job)
    Can "Edit and Deploy New revision" -> Manage revison to split traffic.

GKE (Google Kubernets Engine):

    It consists of,
        Create cluster:
            Node pools - create no of nodes.
        Workload deployment:
            create pods and deployments. 
        Expose application:
            Create a services type and expose the applications.
    
Compute Engine:

    General purpose computing machine.
    2 ways of deployment,
        containarized app
        non containarized app.
    Can install startup script (user data in aws)

Instance Template:

    It is like launch template/launch configuration in AWS.
    Create instance from template.
    
Instance Group:

    This is Auto Sacling Group (ASG) in AWS. 
    Used to manage group of instances.
    It consists of,
        New managed instance group (stateless):
                Give min and max no of instances.
                All machine family type is same. 
        New managed instance group (stateful):
                Give NO min and max no of instances but can select no of instances. 
        New unmanaged instance group:
                Manually manage group of load balancing VMs.
                Can select diff vm machine family type. 
                
Load balancer:
    
    create a load balancer type (http, tcp, udp)
    In backend service, select the instance group.


-------------------------------------------------------------------------------------------------------
PIPELINES:
-------------------------------------------------------------------------------------------------------

Source Code Management:
    Cloud Source Repository.

Build:
    Cloud Build: 
        Trigger based on event.
        Source code repository and branch. 
        Configuration Type: 
            Support yaml/json (cloudbuild.yaml)
            dockerfile 
            buildpacks 
        Incase of dockerfile, choose the registry name. 
        Can run manual to trigger.
        https://cloud.google.com/build/docs/build-config-file-schema

Artifact Storage: 
    Artifact Registry 

Deploy:
    Cloud Deploy 

-------------------------------------------------------------------------------------------------------

Pipeline 1 (Create docker image and push to registry):

    Step1: Setup a source repository
        Cloud Source Repository -> Select project ID  & Repo name.
        Once repo is created, it can clone repo and push the code to repo.

    Step2: Setup a cloud build 
        Select type as dockerfile and source as repository.
        Once build completed, image placed in container registry.

-------------------------------------------------------------------------------------------------------        

Pipeline 2 (Deploy python app to App Engine):

    Step1: Setup a source repository. 
    
    Step2:  Create a cloudbuild.yaml file and app.yaml for app engine.

        cloudbuild.yaml 
        steps:
        - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
        entrypoint: 'bash'
        args: ['-c','gcloud config set app/cloud_build_timeout 1600 && gcloud app deploy']
        options:
        logging: CLOUD_LOGGING_ONLY
        timeout: 1600s

        app.yaml 
        runtime: python311 # or another supported version

-------------------------------------------------------------------------------------------------------                

Pipeline 3 (Deploy to Google Cloud Function):

    Step1: Setup a source repository. 

    Step2:  It requires, 
                cloudbuild.yaml 
                function-source.zip 

    cloudbuild.yaml
    steps:
    -   name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
        args:
        - gcloud
        - functions
        - deploy
        - home
        - --region=us-central1
        - --source=.
        - --trigger-http
        - --runtime=python37
        - --allow-unauthenticated
    options:
    logging: CLOUD_LOGGING_ONLY
            
-------------------------------------------------------------------------------------------------------                

Pipeline 4 (Deploy to google run):

    Step1: create a source repo. 

    Step 2: It requires, 
        Dockerfile
        Source code 
        cloudbuild.yaml 

        cloudbuild.yaml
        steps:
        # build image
        - name: 'gcr.io/cloud-builders/docker'
          args: ['build','-t','gcr.io/gcp-devops-learn-381106/runimage','.']
        #push image to registry
        - name: 'gcr.io/cloud-builders/docker'
          args: ['push','gcr.io/gcp-devops-learn-381106/runimage']
        #deploy to cloudrun
        - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
          entrypoint: gcloud
          args: ['run','deploy','myfirstrun','--image','gcp-devops-learn-381106/runimage','--region','us-central1','allow-unauthenticated']
        images:
        - gcr.io/gcp-devops-learn-381106/runimage
        
-------------------------------------------------------------------------------------------------------                

Pipeline 5 (Deploy to GKE):

    Step 1:
        Create GKE cluster and some sample images and do workload (deployment)
        Create load balancer service. 

    Step2:
        Create source code repo 
    
    Step3: 
        cloudbuild.yaml 
        steps:
        - name: 'gcr.io/cloud-builders/git'
          args: ['clone','repo-path']
        - name: 'gcr.io/cloud-builders/docker'
          args: ['build']
        - name: 'gcr.io/cloud-builders/docker'
          args: ['push']
        substitutions:
          PROJECT_ID: 
          CINAME: 
          VERSIOn: 
        #Below only updates, image name to existing deployment. 
        - name: 'gcr.io/cloud-builders/kubectl'
          args: [kubectl set image deploy deploy-name image-name]
          env: 
          - 'CLOUDSDK_COMPUTE_ZONE=useast-1a'
          - 'CLOUDSDK_CONTAINER_CLUSTER=cluster-name'
        

-------------------------------------------------------------------------------------------------------
JENKINS:
-------------------------------------------------------------------------------------------------------

To integrate with Jenkins:

From "Deployment Manager", can search in market place and install tools like jenkins, wordpress etc. 

    1. Can search Jenkins and select "Google click to deploy" from Marketplace. 
    2. It creates VM using Jenkins Image.
    3. In underlying uses Compute engine, Cloud deployment manager and Cloud runtime.
    4. Input disk size and other parameters.
    5. Once deployed, it will give URL and credentials to login. 
    
-------------------------------------------------------------------------------------------------------
SECURE PIPELINE:
-------------------------------------------------------------------------------------------------------

Secure Container Deployment: 

    1. can use Google base image: 
            Google base image can found in market place. 
            Google uses this base image for their own application deployment. 
    2. Container Analysis.
    3. Binary Authorization.
    
Container Scanning API:
    
    Works same in both artifact registry and container registry. 
    Automated vulnarability scanning:
        whenever image is uploaded, it does auto scanning. 
    Manual vulnarability scanning: 
        gcloud artifacts docker images scan imageurl --remote 
        
    Container Registry -> Settings -> Vulnarability scanning -> Turn ON : Existing images will not be scanned but only new images uploaded.
    
    
Binary Authorization: 

    How to prevent deployment if any vulnarabilities. 
    It is deploy time security control. 
    It applies on GKE & CLoud Run.
    It is policy to maintain. 
    Security -> Binary authorization -> Enable (first time)
    Default Rule: 
        Allow all images
        Disallow all images
        Require attestations - allow only verified by attestors.
            1. Create attestator.
                    require public key (create using kms)
            2. Authorize the image using gcloud command. 
        
    Configure Binary Authorization on Cloud Run:

            While create cloud run, under security require to enable "binary auth".

    Configure Binary Authorization on GKE:

            Cluster -> Security -> Enable Binary Authorization.

    
-------------------------------------------------------------------------------------------------------
CLOUD MONITORING SERVICE:
-------------------------------------------------------------------------------------------------------

CloudOps Tools:

    Logging and Monitoring of applications.

    Monitoring -> Dashboards -> Can create custom dashboard or use google dashboard. 

    Ops agent is require to capture additional information like memory. 

Installing Ops Agent in VM: 

    https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/installation

Create a custom dashboard:

    Monitoring -> Dashboard -> Create dashboard. 
    For each chart, have one metrics tagged. 
    Can place multiple charts. 
    
Setup Uptime Check:
    
    Used to check whether system application is up and running.
    It consists of,
        target 
        health check
        Notification
     
     Alert Policy:
            Create a alert condition based on metrics and threshold and then send notification. 
            
Group: 
    It helps to group resources and apply monitoring.
    
    
-------------------------------------------------------------------------------------------------------
OPS/DEV TOOLS:
-------------------------------------------------------------------------------------------------------    
    
Cloud Error reporting:
    It helps to detect error in application.
    Operations -> Error Reporting.
    Can send to notification/.
    It captures error ex: divide by zero, index outof range etc. 

Cloud Debugger:
    It helps to debug the code if any error.  
    It requires to handle in code to add required libraries.
    Operations -> Debugger -> Add source code.
    Can "take snapshot" on particular line of code. 
    Can "log point history" on particular line of code. 
    
Cloud Trace:
    It helps to find the latency for the google services. 
    
Cloud Profiler:
    It helps to find how much reosurce is consumed.

-------------------------------------------------------------------------------------------------------
OPTIMIZE SERVICE PERFORMANCE:
-------------------------------------------------------------------------------------------------------    
    
Cloud Billing:

    Projects should associated with billing account.
    By default, create billing account and projects are associated. 
    New billing accounts can be created and existing projects can be modified.

    Billing -> Billing Account -> Manage Billing Accounts. 

    Billing -> Cost management -> report 


Pre-emptiable Virtual Machine (Spot): 

    Short lived cheaper VM 
    Can provision preempible VM when, 
        Workload is fault tolerant. 
        Not require 100% available. 
        Cost is less. 
        Upto 80% discount. 
        Max life is 24 hrs. 
        Gives 30s warning before shutdown. 
    Regular VM has higher priority than regular VMs.

    Compute Machine -> Management -> Availability Policy -> Spot 


Compute Engine - Flat rate, Committed use, Sustained use discount: 

    Flat rate - Pay for what you use. 

    Committed use discount (CUD): 
        If workload is predictable. 
        Can commit for 1 or 3 years.
        Only on VM & GKE. 
        Cannot cancel commitments. 
        It consists of hardware and software license commitment. 
    
    Sustained use discount - automated discounts means more use get more discount for a month. Applies to N1 & N2 machine type. Only on VM & GKE. 


Total Cost of Operations (TCO):

    TCO = Cost of purchase + cost of operations. 


-------------------------------------------------------------------------------------------------------
INFRASTRUCTURE DEPLOYMENT:
-------------------------------------------------------------------------------------------------------    

Infrastructure as Code (IaC):

    Create infra using code. 

Terraform:

    Cloud neutral tools which can use to create infra for any vendors like aws, gcp etc .
    In gcloud, terraform is already installed. 

Authentication: 

    https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/getting_started

    Using Json keys:
        1. From service account, create the json keys.
        2. Upload the json keys in gcloud 
        3. In provider block, use the credentials property. 

                provider "google" {
                # Configuration options
                project = "gcp-devops-learn-381106"
                region  = "us-central1"
                zone    = "us-central1-a"
                credentials = "keys.json"
                }


-------------------------------------------------------------------------------------------------------
CLOUD LOGGING: 
-------------------------------------------------------------------------------------------------------    
        
Log Explorer: 
    It allows to search logs based on resource types and other filters. 
    Can write own query to filter. 
    Can download logs and export to other service.
    
Types of Cloud Auditing Log:
    Who did what, when, where
    
    Admin activity: 
        By default enabled. 
        Capture administrative activities. 
        Retains for 400 days.
        Free of cost. 
        Create/delete VM etc. 
        
    Data Access:
        Not enabled by default 
        Create/Modify resource data
        Data retention for 30 days
        Free of cost.
        Creaet object in storage etc. 
    
    System Event:  
        Enabled by default. 
        It is because of google system action.
        Free of costs
        Retains for 400 days.
        Preemptive VM shutdown logs etc. 
        
    Policy Denied:
        If any policy not enabled then throw error.
        
Admin Activity Logs:
    Log Explorer -> Log name -> Admin logs 

System Logs:
    Log Explorer -> Log name -> System logs 

Data Access Logs:
    To enable logs: IAM & Admin -> Audit Logs -> Services (Admin Read, Data read, Data write) -> Enable
    Log Explorer -> Log name -> data_access logs 
    
Log Collection GCloud SDK:
    
    gcloud logging write log-name message 
    gcloud logging write log-name {key=value} --payload-type=json --severity=CRTITICAL    
    gcloud logging read log-name (Filter can based on time, resource, crtiticality etc) 
    
Log collection - Other google cloud services: 
    No need of enable and capture it.
    In log explorere, filter based on service. 
    
Log collection - Install cloud logging agent: 
    Legacy agent (old)
    Opsagent (latest) 
    Require agent to install in VM to capture additional information like memory etc. 
    Internally uses fluentd to collect and send logs.
    
Log collection - Cloud Logging API:
    Python/Java SDK:
        https://github.com/googleapis/python-logging
        
    From Onpremise: 
        Step1: Requires json key from service account. 
        Step2: SET GOOGLE_APPLICATION_CREDENTIALS=key.json  (for authentication) 
        Step3: Install logging using,
                https://pypi.org/project/google-cloud-logging/
             
Log Based Metrics Counter: 

    Logging -> log based metrics -> Create Metric 
    Types,
        Counter: Count no of log entries
        Distribution: Collects numeric data from log entries. Add the field and value.
        
     1. Get the filter query.
     2. Create as Counter or Distribution
     3. Monitoring -> Monitoring Explorer -> To check metrics logs.
     
         
Cloud Log Router: 
    It is used to send the log to cloud storage bucket, cloud logging bucket, pub/sub, big query, splunk etc.
    Logging -> Log router -> Create Sink
        Sink Name, Sink destination (bucket etc), filter query
        
    Logs -> Logging API -> _default (30 days and configurable) , _required (400days retention and unconfigurable), _user defined (30 days retention and configurable) -> pub/sub, big query, splunk etc
    
    Logs Storage: 
        It is cloud logging bucket. 
        It consists of  _default (30 days and configurable) , _required (400days retention and unconfigurable).
        Can create user defined bucket.
        
-------------------------------------------------------------------------------------------------------                    
